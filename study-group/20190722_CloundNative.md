# Clound Native Tokyo 2019

## オイラ大地のECサイトのマイクロサービス化をVA Linuxが性能分析してみた

- Jaeget とか tracingかっこいい
- システムの負荷と合わせて分析すると何か見えるはず
- オイラ大地からの依頼があって実データを分析を始めた

- 分析するとわかること
  - ボトルネックがあって性能が出ていなかったり、無駄にリソースにお金を支払っていないですよね？

- メインのサービスは「オイシックスクラブ」
  - サブスクリプションモデルが主体
  - お好みの商品の変更
  - キャンセルを自由に

- これまでのマイクロサービスの取り組み
  - 伝統的なモノシリック構成
  - この問題に対応するためにマイクロサービス化に着手
  - 開発環境
    - win / mac どちらでもOK
    - Gradleが動けがIDEA, Eclipseどちらでもいい
    - JavaかKotlin
    - API定義はSwagger

- CI/CDはGitOps
  - CircleCIで実現している
  - customization.yamlでk8sのyamlを編集している
  - Docker HubにDocker Imageを push し k8sで展開している

- IDCF Clound - VM - Oracle(オンプレミス)
  - Strage Queue, Azure MyuSQL, Azure Redis Cache
  - 方針
    - ストラングラーパターン
      - 既存のシステムを徐々に切り出しコンテナ化している
    - IDCF Clound と Azure を使用したマルチクラウド構成
    - 共有データベースの段階的な分離

- 今回、分析したサービス
  - 定期ボックス作成バッチ
  - 木曜日を起点に動作している
    - 顧客が自由にカートの編集できるための準備
    - 数十万件
    - ユーザごとにカスタマイズしていくので、まあまあ大変
    - バッチとオペレーションがぎっしりしている

- 想定以上のお客様の増加によりバッチが不安定化
  - 1インスタン構成でのバッチのため全体的に遅くなった

- 試行錯誤
  - VMで動作していたのをAzureのAKSに移動
  - オンプレのOracleとネットワーク的な距離が遠い
  - リフトアンドシフトは断念
  - モノリスをOSごと丸々コピーした２台目を作成
    - VMのスペックをあげまくる
    - 問題にお金で対応して、急場をとりあえずしのげる状態にした
  - 暫定対応を行ったので、マイクロサービス化に再挑戦
    - バッチの前半にOracleのリソースを大量に利用するSQLが多かった
    - 不安定になるのは前半部分のSQL結果をマルチスレッドを実行している後半部分
  - クラスタ上で処理計測を行うノウハウが足りなかったのでVA Linuxさんに依頼した

- 性能測定
  - もらったのはテキスト形式
    - タイムスタンプ
    - POD名
    - ログレベル
    - スレッドID
  - 興味があるのは
    - 時間あたりの処理件数
    - 1件あたりの処理にかかる時間
  - グラフにしてみると
    - 空白時間がある
    - 数十万件の定期ボックスを300件づづに分解し、マルチスレッド化
    - お客様によって、多数の商品を希望することがあり、その場合は次のスレッドが起動するまで待ち状態になる
  - 応答時間
    - バッチ1件あたりの処理時間
    - 同時処理件数 vs 応答時間
      - 処理件数の増減の影響を受けていなかった
    - Pod単位で20並列、Pod全体で100並列
  - 注文件数で回帰
  - 応答時間
    - 回帰残差
  - system metrixを見ても遅い原因がよくわからなかった
    - 遅い並列処理があることで並列数が一時的に遅く見える
  - 処理レートの低下要因
    - 並列度が十分高くない
    - 遅い処理が影響している
      - 外れ値
  - 注文件数から予測される処理時間との差(積算)
  - イエーガというOSSで分析ができる

- まとめ
  - 処理レートグラフ -> スレッドへのジョブの振り分けを改善
  - 応答時間 -> 遅い時間帯が見つかったけどよくわからない
  - モノリスのどこをマイクロサービスかするのかの見極めが重要

## k8sでJava / JVM アプリを動かすノウハウ

- イントロダクション
  - コンテナワークロードに対する一般的な期待
    - 1コンテナ1プロセス
  - JVMの考え方とコンテナと相性が良くない面もある
    - Java EEを代表するアプリケーションサーバに
  - Clound Nativeの世界に置いてJVMはオワコンか？

- JVMコンテナアプリを開発作業を楽にしよう
  - Java on k8sのランドスケープ
    - ローカル開発
      - 開発用のk8s クラスタにデプロイまで動作確認のために行う
      - Jib + Skaffoldを利用すると便利
      - Jib
        - Dockerfileの編集とコンテナのビルドからアプリ開発者を解放する
        - Gradleのプラグインとして利用可能なJVM言語アプリケーション
        - 使い慣れたビルドツールでコンテナイメージが作成できる
        - id("com.google.cloud.tools.jib")
      - Skaffold
        - 面倒なbuild, Push, Deployまでを自動化
        - 開発用のクラスタに乗せてk8sのデプロイを自動化できる
        - plugin アーキテクチャ
        - build, push, deployに当たる各フェーズにコマンドを割り当てて実行する
        - Jibを利用している場合は、build, pushがjibに置き換わる
        - <https://github.com/orisano/dlayer>

    - ビルド
      - JVM固有の事情を考慮しないといけない
      - Dockerfileのベストプラクティスの実現を目指す
        - 余計なファイルを入れない
        - コンテナのフットプリントを小さくするためにJVM固有のノウハウが必要
      - 適切なベースイメージを洗濯しよう
        - <https://k11i.biz/2018/05/17/base-docker-image-for-java>
        - Debian(headless)が最適かと思われる
        - Alpine は JDK12, 13は対応している
          - Project Portola
            - Alpineを利用してJVMコンテナの最適化を目指すプロジェクト
            - これが利用できると素晴らしいけど、まだまだ開発中
        - カスタムJREを作成してサイズを落とそう
        - Dockerfile内のマルチステージビルドでカスタムJREを作成する
        - より発展的なJVMコンテナのビルド手法
          - Jib
          - GraalVM
          - SubstrateVM
            - 普通はやちゃダメな黒魔術的なやつ
            - JVMを必要なコンポーネントだけに絞って自力でビルドする

    - デプロイ
      - コンテナのリソース制限と暖機運転に注意する
      - リソース制限
        - JDKのバージョンが低いとコンテナ上ではなく全リソースを対象として検知されてしまう
        - GC
          - CPUの割り当て制限
          - JDKが認識してくれるコンテナのCPU制限の設定
            - --cpus, --cpu-shares, --cpu-quota
            - コンテナ状のJVMだとsharesしか認識されない
          - コンテナに設定したリソース設定をJVMは正しく認識されるようになった
          - JDK11で1CPU, 512MBのコンテナに割り当ててJVMのデフォルトに任せるとシリアルGCが選択される
            - シンプルなGCが選択されるとFull GCが起動されやすくなってしまう。
            - リソースの割り当て量とGCアルゴリズムの選択は古くから議論
        - ロールアウトで気にすべきこと
          - JVMアプリケーションが最大の性能を発揮するには暖機運転が必要
            - JVMは実行中のプロファイルを参考にしながらｍアプリケーションのコードを動的に求めいる
          - 暖機運転が済んでいないJVMコンテナをうんちゃら
          - 暖機運転の自動化
            - Podが起動したらReadiness ProbeがOKとなる前にサイドカーコンテナから暖機運転のトラフィックを送る
            - スライドを再確認
            - 1万回には理由があり、最適なバイトコードには呼び出すにあたり必要
        - Servivce Meshによるトラフィックの制御
          - IstioなどのService Methを利用すると、Podに流すトラフィックの割合を1%単位で調整できる

    - 監視
      - k8sで標準の監視ツールが利用できる
        - jcmd, jstackなどの基本ツール
        - JFR (Java Fright Recoder)
          - プロファイリングツールの１つ
          - JMC(GUIツール)による監視ができる
          - 永続化している場所に吐き出す必要がある
          - あとでスライドを見て、JFRの利用方法を確認する
          - JFRのファイルをAPIで参照することもできる
            - プロメテウスに擬似的にリアルタイム反映するようなこともできる
        - カスタムJREにすると除外されてしまう
        - サイドカーとしてJDKのコンテナを追加して、そのコンテナからツールを実行する
      - Pod上のコンテナ間でプロセスが参照できるように設定する必要がある
# 20190618 k8s meetup kubecon recap

## KubeCon + CloudNativeCon Europe 2019 Overview

- keynote
  - CNCF Projectアップデート情報
  - OpenTelementry
    - OpenTracing. OpenCensusを統合した
    - 次のバージョンからマイグレーションされる
  - Service Meth Interface ? (SMI)
    - Microsoftのスポンサーセッションで発表
    - Interfaceが共通化されわかりやすくなる
    - Ingressに近い形になるはず
    - LINKERD, Redhat, Racnherなどが参加を表明している

- CNCF
  - Helm
    - v3でTillerがなくなる
  - Harbor 1.8
    - OpenIDConnect対応、ロボットアカウント対応

- index-free log
  - OK Log, kubectl logs. grafana loki
    - OKLogが開発終了？
  - Grafana Loki
    - Prometheusのようなログあグリケーションシステム
    - Aißrplain ModeとMicroservices Modeががある

## 5G Orchestration in Serverless & Managing Edge Computing with Serverles

- 5G Orchestration in Serverless
  - 5Gからのデータをcloud側で処理するのは問題
    - IoTでよく話される内容
    - 5Gのでデータを直接Cloundに残すのは問題？
  - Edge側で処理をしよう
  - Edge側で処理のアプリケーションはコンテナではなくFaaSにしよう
  - Edgeを１つの大きな分散コンピューティング
  - データはEdgeネットワークのどこでも利用可能
  - Edgeネットワークのどこでもワークロードを実行する

- Akraino Edge Statck
  - EdgeのOSSフレームワーク
  - マイクロサービス間のインテグレーションやコンテナが大きいことが問題になることがある
  - Functionごとにデプロイできるのがいいよね
  - MQTTプロトコルでCloundでやり取りする
  - HTTPプロトコルに比べHdeaderサイズが小さい
  - Kafka経由でFaaSが呼ばれる

- ONAP
  - EdgeのクラウドPoPとONAPで通信する？

- 活用事例
  - 渋滞状況を管理しているシステム


- Managing Edge Computing with Serverles
  - データをCloundのみで処理をするのは問題外ある
  - Intelligence to the Edge

- Nuclio
  - Non blockingで並列で実行される

- CloundとEdgeはMQTTで通信するのが一般的に
- Edge側でServerless

- 工場に配置したエッジデバイス(ラズパイとか)
  - IoT gateway- MQTT-> Clound IoT Core
  - AWSとAzureのIoTのサービスを検証してGCPとk3sの構成に
    - 政治的にGCPを理由
 

## Modern CI/CD with Tekton and Prow Automated via Jenkins X

- 今日はTektonの話が中心
- Continuous Delivery Foundationが立ち上がった
- Linux Foundationの下位組織
- Jenkinx XはJenkinsが必要になる
  - Jenkins パイプラインが必要

- k8s ネイティブであること
  - あらゆるプロセスはコンテナとして実行できる
  - オブジェクトはDeclarativeに記述される
  - 状態の変更は拡張可能なk8s Apiを通じて行われる

- Tekton
  - Knative-build-pipelineがTektonになった
  - 足りていないもの
    - Event Triggering
    - Log Persistence
    - SCM support
      - Githubのみ対応している
      - chatopsとかはできていない
    - Pipline Resouce / Taskの拡張性がない
  - Tektonだけでは色々と解決しない


## Kubectl Apply 2019: Defense Against the Dark Arts

- 闇の魔術に対する防衛術(Defense Against the Dark Arts)
  - Kubectrl applyの裏側で何が起きているのか？
    - 全てのリクエストはAPI Server経由する
    - etcdがSingle Source of Truth
      - etcdが全て持っている
    - 設定項目の妥当性
      - dry-runを使う？
        - クライアントサイドにてAPIに実際に渡すリクエストを生成するコマンド
        - いわゆるdry-runと違いAPIサーバとはやり取りしない
    - k8sの変更
      - 各リソースに対してApplysjるためのエンドポイントが増えた
      - 直近で適用されたConfigとのdiffが取れるようになった
      - kubctl diffコマンドができた
        - いい感じのやつ
        - 1.13にリリースされている(EKSではまだ使えない)
    - k8sの今後の変更
      - Pruneの改善
        - いい感じにリソースを削除してくれるコマンド
  - 実践
    - Applyの怖さ
      - 正直どうやって動いているのかよくわからず使っている
      - 下手に共通化すると変数の管理とか大変
      - 設計ミスるとやばい
      - 環境 * サービスが増えるとyaml Hellに陥る
    - YAMLの管理をどうやったらいいかがわからなくなる
    - GitOpsはまだまだ茨の道
    - CDツールを新しく構築し直すコストに見合わない場合がある
    - kubectl diffが齎すもの
      - メリット
        - CIツール内でdiffとってログが残せるようになった
        - 最短で1分で構築できる    - 


## Production GPU Clusters With K8s for AI and DL workloads

- 


## Resize Your Pods w/o Disruptions... & Restart-Free Vertical Scaling...

- pod 
  - Podが使用するリソース量を指定して予約することができる
  - この指定は多くても少なくてもだめ
    - 実際にやっている人は少ない

- VerticalPodAutocscaler
  - podの垂直スケーリングを自動的に行うための仕組み
  - 実際のリソース利用率から値を計算してresource.requestsを設定する
  - 現在はまだベータ
  - resouce.requestsの変更するする場合はpodの再起動が必要
    - 再起動しないで適用される仕様が検討中

- 長時間稼働するジョブをelastic podにすることで50から60% costを削減することができる

- scheduling in action
- これらに変更を加えていく予定
  - dcheduler
  - kubelet 
  - ?

## イアンさんが緊急LT

- Runtime Trackのセッション紹介
  - container runtimeとは？
    - コンテナを実行してくれるやつ
    - docker -> containerd -> runc
      - どれがランタイムなの？
    - ランタイムがやっていること
      - image build
      - image management
      - run container
      - stop container
      - delete container
      - inspect container
      - remote api
    - Run linux container
      - setup namespaces
      - setup cgroups
      - setup mounts
      - set $home
    - runtimeをレベル分けして考える
      - High-lebel runtime (dockerやcontainerd)
      - low-level runtime (runc)
        - rust-vmm
        - コンテナをvmの中で実行すること
    - Containerd v2 shim api (@estesp)
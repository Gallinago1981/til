# 20181218 Apache Kafka Meetup 2018

## Real Time Stream Processing with KSQL and Kafka

- Changeing Architectures
  - Data Wherehouse - Data Lake -> Data Streaming
  - BigDataはデータ量に比例して増えていく
  - Stream dataは順次処理をしていくので減っていく
  - KAPPA -> アーキテクチャ
  - DatabaseのLogをリアルタイムでStreaming Platform, DatabaseのLogテーブルに格納できる
- Kafka?
  - publishとｃsubcribeを領しKVSのデータストアへの反映をリアルタイムで更新できる
  - 
- Stream Processing
- KSQL
  - KSQL(Processing)
- KSQL in usecase


## LINE Ads Platformの開発を支えるKafka

- LINE ads Platform
  - 運用型広告
    - 広告主が広告料や内容をリアルタイムで更新していくサービス
- LAPでのKafkaの活用
  - かくコンポーネント間でのデータの受け渡しでKafkaを利用している
  - 配信に必要となるかく情報をKafkaで収集している
  - 利点
    - 責任範囲を明確にできる
      - 間にkafkaが入ることで開発チームが別れていてもIFさえ決まっていれば各開発に集中できる
      - データの活用にProducerは関与しないため、自由にconsumerを追加できる
      - batchingによる高スループット
      - データがディスク上に保存されるのでconsumerがダウンしても安全に利用できた
- implressionとClickのJoin 


## I/O intensiveなKafka ConsumerアプリケーションのスループットをLINE Ads Platformではどのように改善したか

- Mobile App Segment
  - 広告主が特定の商品を閲覧したユーザに対し広告を配信したい
  - segment master dataを読み込んでredis/HBaseへの更新を行なっている
  - Single Ebent Proessing
  - I/Oによる負荷がほとんど
  - Using Kafka Streams DSL
    - HBase絵の同期描き込もがボトルネックになった
    - HBaseが遅かった訳ではなく、Clientが遅かった(Flusing分、遅れた)
    - Async Write にしたら早くなった？
      - データが損失する可能性がある
    - Stream Treadを使って解決する方法もある
      - 処理がスケールするのでスループットは上がるはず
      - 上限を決める必要がある
        - partitionのsubsetが割り当てられる
        - partition数を増やすと producerやbrokerのパフォーマンスに影響
        - IMF Kafka Clusterではcぅsてrに置けるぱーてぃしょんペナルティを避けるためpartition数に上限が設けられている
      - Kafka Streamをやめた
        - consumuしたoffsetの状態を管理するバッファを用意した
        - 未完了のoffsetが溜まってきたら処理を一旦止める

## Yahoo! JAPANのKafka Platformで起きた障害とチューニング

- チューニングの実例
  - Request Handlerが重い
    - Request HandlerのIdle%が低い
      - 一部のクラスタで30%を割った
      - 設定の違いによる遅延は見つからなかった
      - offsetの刻み幅は通常のクラスタは刻み幅が大きかったが、問題のあったクラスタはカウントアップだった
      - 問題のあったクラスタのデータの流し方が特殊だった
        - Producer1台あたりのreq/se < Partition数という構成が原因
      - 改善策
        - Producerは数を減らす
          - ビジネス要件的にアウトだった
        - Partitonを減らせないか
          - 動作中のpartition数は減らせなかった
        - linger.msを長くすればいい
          - ビジネス要件で不可能
        - ProducerごとにKeyを設定
          - メッセージのKeyが同一の場合、同じProducerに割り振られるKafkaの仕様を利用した
  - 長時間連続運転すると突然プロセスがダウン
    - File Descriptorの上限を迎えてプロセスダウン
      - FDの解放ができていない
    - lsofコマンドでプロセスが掴んでいるファイル数を調べた
      - 問題のクラスタでは4倍以上
      - 削除済みのFDを抱えていた
      - JVMのGCで解放されるはずが、抱えたまま
      - mixed gcが発生していなかった（問題のあった方）
        - deleted ファイルの解放にはmixied gcが必要と判断
        - req/secが多いクラスタではkafka brokerで確保するオブジェクトの数や容量も多くなる
          - Young GCの発生頻度が高い -> Mixied GCが閾値に当たりやすい
        - req/secが少ない
          - Young GCの発生頻度が低い -> Mixed GCの発生要件を見たさず先にFDが枯渇
          - JVMのパラメータを設定した
            - デフォルトより小さくした
            - InitialngHeapOccupancyんちゃら
    - まとめ
      - 監視が必要
      
# D3 - 自動テスト、テストモデリングについて

## Flakyなテストとその判別方法の解説
### GooleのJohn MiccoしによるFlakyなテスト
* 個人の見解
* 資料はあとでうｐされる
* 基調講演の資料もあるよ
* 記事が長いからスライドにまとめた
* スライドに書いてあるのは基調講演のページ数
* Googleとはいえ特殊なことはしていない

### Googleの自動テストの内容
* presubimit testing
  - 依存関係からテストする
  - コミットごとに分離してテストする
* Postsubmit testing 
  - 420万件のテストはこっち
  - 一定のコミット量が発生したらテストを実行する
  - ２年間結果を保存
   1. テストが失敗した瞬間に原因のコードがわかる
       unit test
       E2Eでは難しい
   2. コミットが入るたびに全てのテストが流れる
   3. コミットが入るたびに影響がある失敗があるテストだけが流れる
   「失敗する可能性が高い」は見つけられ
* 継続的に420万のテストケースが流れている
* 全ての結果がDBに保存されている
* 手動はUXと多言語対応
* RTS(依存関係から対象となるテストケースを見つける)
* 実行したテストの99.8%のテストが成功する
  - 残りの0.2%を実行したい
* グリニッシュサービス
* 各サービスごとにテストを紐づけている
* 決定論ではなく確率論
* Googleはリポジトリが１本(Gmail, Searchで別れていない)
* Skip Tests
  - 必要のないテストは実施しない
  - 影響が高いテストを実行したい
  - 結果が変わるテストの方がきになる
  - スキップ後の失敗は原因がどこにあるかがわからない
    - 成功から失敗が原因特定が容易になる
  - Ggoogleのテスト結果分析
    - 成功から失敗の原因がFlackyな失敗
    - 同じファイルの更新頻度が高い
    - 3人以上が関わると失敗しやすい
    - ペアプロは違つ
  - JavaよりGoの方がテストの失敗は起きにくい
* Flakey
  - 当てにならないテスト
  - 成功と失敗の両方が観測できるテスト
  - 再実行のリソースが多い(2%から16％)
  - 全力で解決したい
  1. テストが失敗したがリトライしたら成功した
  2. テストが成功（失敗）したが次のタイミングでもう一度テストを実行したら失敗（成功）した
    - 結果が変わるのもFlaky Test
    - 簡単に見極める方法

### チュートリアルからみるFlackyの世界
* 資料は未公開
* 公開されそうな予感
* GoogleのテストケースをBigQuer
* テスト結果を分類をし責任者を名明確にする
#### Flackyを見極める
1. Flackyなテスト候補
* より多くの繊維があるテストはFlaky
* 結果がより変わる場合は不安定
* 結果の遷移が多いものをカウントするだけのSQLで判別
  
2. Flacyではないテスト候補
* テスト結果が同じパターンの場合は、Flakyではない

### まとめ
* テスト文化を地道に築き上げた

## お弁当Q&Aのあみだくじ
* コードはほとんど書いていない
* テスト設計の話
* セブンイレブンのプリンタのテスト屋さん
* 時系列のイベントを整理する
* UML/シーケンス図、状態遷移図
  * もっと簡単にやりたい
* イベントが追加しやすい


## 明日から始めるSelendiによるブラウザテスト
### Selenideを使ったブラウザテスト入門
* Selenide
  * Seleniume Web Driverを使いやすくしたもの
  * DSI風にかける
* ブラウザ毎のWebDriverの自動ダウンロード
* import static でSelenideをimportすることで$がそのまま使える
* ＄はメソッド
* $$で複数結果が得られる
* $はページ上で一番最初に出てきたやつ？
### ブラウザテストの面倒くさいこと
### JenkinsなどのCI実行


